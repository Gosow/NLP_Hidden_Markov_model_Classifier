{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP_HMM_VITERBI_ALKASSOUM_YACINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Be carefull, I used Log(probabilities), why? \n",
    "Taking the log not only simplifies the subsequent mathematical analysis, \n",
    "but it also helps numerically because the product of a large number of small \n",
    "probabilities can easily underflow the numerical precision of the computer, \n",
    "and this is resolved by computing instead the sum of the log probabilities."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To learn the HIDEN_MARKOV_MODEL parameters, you can use Baum-Welch implementations, Baum-Welch use both Forward and Backward\n",
    "\n",
    "\n",
    "Before using Viterbi to predict the most viable path, you need all HMM parameters.λ(π,A,B) with\n",
    "- π Initials probabilities ( Enter in the HMM knowing the i state)\n",
    "- A, the transition matrix (Fullfilled with probabilities to go from state i to state j, i&j ∈ hmm states)\n",
    "- B, distribution of probabilities associated to each i state\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I tried to use Smoothing on probabilities using δ(V) delta to reduce correctly the log proba near zero \n",
    "when the token not present in train dataset\n",
    "\n",
    "example of \"résumé\" ,which is not present in Train. It breaks the Viterbi computation, You can fix it with adding a special tag into the taggers dicts, which will refer to no classifiable tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lib and dataset imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "with open(\"./train.ester1.cut.bio\") as file:\n",
    "    content = file.readlines()\n",
    "content = [ elem.strip().split(\" \") for elem in content]\n",
    "\n",
    "# List of sentence\n",
    "file = open(\"train.ester1.cut.bio\", \"r\")\n",
    "train = file.read().split(\"\\n\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleansing and Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de mots defférents: 33085\n",
      "Nombre d etiquettes différents: 15\n"
     ]
    }
   ],
   "source": [
    "# List of tuples/dictionarys\n",
    "wordDiff = {}\n",
    "taggersDiff = {}\n",
    "aTagger = []\n",
    "aMot = []\n",
    "\n",
    "\n",
    "#We split lines into tpl of words: TOKENISATION\n",
    "def splitSentence( sentences ):\n",
    "    sent = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        temp = []\n",
    "        for tpl in sentence.split(\"\\n\"):\n",
    "            temp.append(tpl.split(\" \"))\n",
    "        sent.append(temp)\n",
    "        \n",
    "    return sent\n",
    "\n",
    "for elem in content:\n",
    "    if elem[0] != '':    \n",
    "        wordDiff[elem[0]] = 1 if elem[0] not in wordDiff else wordDiff[elem[0]]  \n",
    "        taggersDiff[elem[1]] = 1 if elem[1] not in taggersDiff else taggersDiff[elem[1]]\n",
    "            \n",
    "\n",
    "for etiq in taggersDiff:\n",
    "    aTagger.append(etiq)\n",
    "    \n",
    "for word in wordDiff:\n",
    "    aMot.append(word)\n",
    "print(\"Nombre de mots defférents: \" + str(len(wordDiff)))\n",
    "print(\"Nombre d etiquettes différents: \" + str(len(taggersDiff)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRIVIAL SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We split lines into tpl of words: TOKENISATION, each word with his tag\n",
    "spl_train=splitSentence(train)\n",
    "#filling a dict of Wordtagzip\n",
    "dic_trivial = {}\n",
    "\n",
    "for i in content :\n",
    "    if i[0] != '':\n",
    "        #Wordtagzip\n",
    "        dic_trivial[(i[0],i[1])] = (dic_trivial[(i[0],i[1])]+1) if (i[0],i[1]) in dic_trivial else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour le mot :midi, l'étiquette est : O\n",
      "Pour le mot :en, l'étiquette est : O\n",
      "Pour le mot :temps, l'étiquette est : O\n",
      "Pour le mot :universel, l'étiquette est : O\n",
      "Pour le mot :quatorze, l'étiquette est : B-time\n",
      "Pour le mot :heures, l'étiquette est : I-time\n",
      "Pour le mot :à, l'étiquette est : O\n",
      "Pour le mot :Paris, l'étiquette est : B-loc\n",
      "Pour le mot :l', l'étiquette est : O\n",
      "Pour le mot :information, l'étiquette est : O\n",
      "Pour le mot :continue, l'étiquette est : O\n",
      "Pour le mot :sur, l'étiquette est : O\n",
      "Pour le mot :RFI, l'étiquette est : B-org\n"
     ]
    }
   ],
   "source": [
    "observations = (\"midi\",\n",
    "                \"en\" ,\n",
    "                \"temps\" ,\n",
    "                \"universel\" ,\n",
    "                \"quatorze\" ,\n",
    "                \"heures\" ,\n",
    "                \"à\" ,\n",
    "                \"Paris\" ,\n",
    "                \"l'\" ,\n",
    "                \"information\" ,\n",
    "                \"continue\" ,\n",
    "                \"sur\" ,\n",
    "                \"RFI\")\n",
    "\"\"\" real :\n",
    "midi O\n",
    "en O\n",
    "temps O\n",
    "universel O\n",
    "quatorze B-time\n",
    "heures I-time\n",
    "à O\n",
    "Paris B-loc\n",
    "l' O\n",
    "information O\n",
    "continue O\n",
    "sur O\n",
    "RFI B-org\n",
    "\"\"\"\n",
    "countWord = []\n",
    "for word in observations:\n",
    "    for wordTrv in dic_trivial.keys():\n",
    "        if wordTrv[0] == word:\n",
    "            countWord.append([dic_trivial[(wordTrv[0],wordTrv[1])],wordTrv[1]])\n",
    "\n",
    "    m = max(countWord)\n",
    "    print(\"Pour le mot :\" + word + \", l'étiquette est :\", m[1])\n",
    "    countWord = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN\n",
    "For Viterbi Algorithm, we need the 3 parameters of the HMM:\n",
    "λ(π,A,B) with\n",
    "- π Initials probabilities ( Enter in the HMM knowing the i state)\n",
    "- A, the transition matrix (Fullfilled with probabilities to go from state i to state j, i&j ∈ hmm states)\n",
    "- B, distribution of probabilities associated to each i state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## π Initials probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log(proba) : \n",
      "\t O: 0.268\n",
      "\t B-org: -1.536\n",
      "\t I-org: -4.877\n",
      "\t B-time: -1.743\n",
      "\t I-time: -4.877\n",
      "\t B-func: -2.305\n",
      "\t I-func: -4.877\n",
      "\t B-pers: -1.154\n",
      "\t I-pers: -4.877\n",
      "\t B-loc: -1.795\n",
      "\t I-loc: -4.877\n",
      "\t B-amount: -2.308\n",
      "\t I-amount: -4.877\n",
      "\t B-prod: -2.67\n",
      "\t I-prod: -4.877\n"
     ]
    }
   ],
   "source": [
    "# List of firts taggers of each sentence\n",
    "lst_firsts_taggers = []\n",
    "lst_firsts_probs = []\n",
    "delta=0.5#SMOOTHING\n",
    "    \n",
    "for sentence in spl_train:\n",
    "    # firts taggers of each sentence\n",
    "    lst_firsts_taggers.append(sentence[0][1])\n",
    "\n",
    "# count of sentence in train\n",
    "nb_sentence = len(lst_firsts_taggers) \n",
    "\n",
    "# Log(P) of each tagger\n",
    "for e in aTagger:\n",
    "    # Adding smoothing\n",
    "    p = (delta+(lst_firsts_taggers.count(e)))/((delta*nb_sentence)+len(aTagger))\n",
    "    p = math.log10(p) \n",
    "    lst_firsts_probs.append(p)\n",
    "\n",
    "# final dic for PI\n",
    "pi = dict(zip(aTagger,lst_firsts_probs))\n",
    "\n",
    "print(\"Log(proba) : \")\n",
    "for e,p in pi.items():\n",
    "    print(\"\\t\", str(e) + \": \"+ str(round(p,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A, the transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "aDic = []\n",
    "# for each tag ,dict\n",
    "for tag in aTagger:\n",
    "    probas = [0] * len(aTagger)\n",
    "    dic = dict(zip(aTagger,probas))\n",
    "\n",
    "    for sentence in spl_train:\n",
    "        # in case the last tupl is empty\n",
    "        nb= 2 if sentence[(len(sentence)-1)] == [''] else 1\n",
    "        end = (len(sentence)-nb)\n",
    "        #  On each sentence, number of Trasition between each tags\n",
    "        for i in range(0, end):\n",
    "            if sentence[i][1] == tag:\n",
    "                dic[sentence[i+1][1]] += 1\n",
    "    for d in dic:\n",
    "        dic[d] += delta\n",
    "    tot = sum(dic.values())\n",
    "    for et in dic:\n",
    "        dic[et] = math.log10((delta+dic[et])/(tot+len(taggersDiff)))\n",
    "\n",
    "    aDic.append(dic)\n",
    "\n",
    "# Final dict, transition matrix A\n",
    "A = dict(zip(aTagger,aDic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B, distribution of probabilities associated to each i state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "aDic = []\n",
    "# dict for each tagger\n",
    "for tag in aTagger:\n",
    "\n",
    "    probas = [0] * len(aMot)\n",
    "    dic = dict(zip(aMot,probas))\n",
    "    \n",
    "    for sentence in spl_train: \n",
    "        for tpl in sentence:\n",
    "            if tpl != ['']:\n",
    "                if tpl[1] == tag:\n",
    "                    dic[tpl[0]] += 1\n",
    "    for d in dic:\n",
    "        dic[d] += 1\n",
    "    tot = sum(dic.values())\n",
    "    for et in dic:\n",
    "        dic[et] = math.log10((delta+dic[et])/(tot+len(taggersDiff)))\n",
    "    aDic.append(dic)    \n",
    "\n",
    "emission = dict(zip(aTagger,aDic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VITERBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have all our HMM parameters, Let s try it with Viterbi's Algorithm\n",
    "#let's try it on dev dataset\n",
    "\n",
    "\n",
    "file_dev = open(\"dev.ester1.cut.bio\", \"r\")\n",
    "dev = file_dev.read().split(\"\\n\\n\")\n",
    "devSplit = splitSentence(dev)\n",
    "file_dev.close()\n",
    "\n",
    "def error_sq(seq1,seq2 ):\n",
    "    if(len(seq1)!=len(seq2)):\n",
    "        return -1\n",
    "    if(len(seq1)==0):\n",
    "        return -1\n",
    "    else:\n",
    "        nb_true=0\n",
    "        for i in range(0,len(seq1)):\n",
    "            if(seq1[i]==seq2[i]):\n",
    "                nb_true+=1\n",
    "        return (1-(nb_true/len(seq1))) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi is such simple, it's the product max between all Φt-1(i) *transition prob*  current tag prob\n",
    "It works as Forward Algorithm, but, Forward use Sum, Viterbi use the product\n",
    "We have 2 lists:\n",
    "    -List of probabilities(j=max * i * transition * taggerconsumption), with j= current proba,i the previous layer one\n",
    "    -List for the path wich gave us the max proba, We fill it when BACKTRACKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi( unePhrase ):\n",
    "    p_ath = []\n",
    "    aObservations = []\n",
    "    aTaggers = []\n",
    "    aViterbiProba = []\n",
    "    aViterbiTags = []\n",
    "    lastDic = {}\n",
    "    for word in unePhrase:\n",
    "        aObservations.append( word[0] )\n",
    "        aTaggers.append( word[1] )\n",
    "    #We search for the list of taggers which have the best prob for our Observaation\n",
    "    for obs in aObservations:\n",
    "        dicProba = {}\n",
    "        dicEti = {}\n",
    "\n",
    "        # we take the value in PI on the tagger(i) if we just enter the graph\n",
    "        if aObservations.index(obs) == 0:\n",
    "            for et in aTagger:\n",
    "                dicProba[et] = pi[et] + emission[et][obs]\n",
    "                dicEti[et] = \"-\"            \n",
    "        else:\n",
    "            #else we do the product max between all Φt-1(i) transition with the current tag\n",
    "            for et in aTagger:\n",
    "                    probTemp = {}\n",
    "                    try:\n",
    "                        for tag in aTagger:\n",
    "                            probTemp[tag] = ( lastDic[tag] + A[tag][et] + emission[et][obs] )\n",
    "                        dicProba[et] = max(zip(probTemp.values(), probTemp.keys()))[0]\n",
    "                        dicEti[et] = max(zip(probTemp.values(), probTemp.keys()))[1]\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "        aViterbiProba.append(dicProba)\n",
    "        aViterbiTags.append(dicEti)\n",
    "        lastDic = dicProba\n",
    "    \n",
    "    # BACKTRACKING TO GET THE BEST PATH, the one which gave us the max proba to get the observation\n",
    "    i = len(aViterbiProba) - 1\n",
    "    tag = \"\"\n",
    "    while i >= 0:\n",
    "        if i == len(aViterbiProba) - 1:\n",
    "            dic = aViterbiProba[i]\n",
    "            tag = max(zip(dic.values(), dic.keys()))[1]\n",
    "            p_ath.append(tag)\n",
    "        else:\n",
    "            p_ath.append(aViterbiTags[i+1][tag])\n",
    "            \n",
    "        i -= 1\n",
    "\n",
    "    #reversing the path       \n",
    "    p_ath = p_ath[::-1]\n",
    "    \n",
    "    print( \"\\nChemin prédis:\\t\", p_ath, \"\\nChemin réel:\\t\", aTaggers )\n",
    "    \n",
    "    \n",
    "    return round(error_sq( p_ath, aTaggers ), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La sentence : \n",
      "midi en temps universel quatorze heures à Paris l' information continue sur RFI \n",
      "\n",
      "Chemin prédis:\t ['O', 'O', 'O', 'O', 'O', 'I-time', 'O', 'B-loc', 'O', 'O', 'O', 'O', 'B-org'] \n",
      "Chemin réel:\t ['B-time', 'O', 'O', 'O', 'B-time', 'I-time', 'O', 'B-loc', 'O', 'O', 'O', 'O', 'B-org']\n",
      "15.38 % d'erreur\n",
      "\n",
      "\n",
      "La sentence : \n",
      "c' est avec Philippe Lecaplain bonjour \n",
      "\n",
      "Chemin prédis:\t ['O', 'O', 'O', 'B-pers', 'I-pers', 'O'] \n",
      "Chemin réel:\t ['O', 'O', 'O', 'B-pers', 'I-pers', 'O']\n",
      "0.0 % d'erreur\n",
      "\n",
      "\n",
      "La sentence : \n",
      "bonjour à tous \n",
      "\n",
      "Chemin prédis:\t ['O', 'O', 'O'] \n",
      "Chemin réel:\t ['O', 'O', 'O']\n",
      "0.0 % d'erreur\n",
      "\n",
      "\n",
      "La sentence : \n",
      "à la une de l' actualité \n",
      "\n",
      "Chemin prédis:\t ['O', 'O', 'O', 'O', 'O', 'O'] \n",
      "Chemin réel:\t ['O', 'O', 'O', 'O', 'O', 'O']\n",
      "0.0 % d'erreur\n",
      "\n",
      "\n",
      "La sentence : \n",
      "l' épidémie de pneumonie atypique \n",
      "\n",
      "Chemin prédis:\t ['O', 'O', 'O', 'O', 'O'] \n",
      "Chemin réel:\t ['O', 'O', 'O', 'O', 'O']\n",
      "0.0 % d'erreur\n",
      "\n",
      "\n",
      "La sentence : \n",
      "au cours de ces dernières heures huit personnes ont succombé à la maladie en Chine \n",
      "\n",
      "Chemin prédis:\t ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-loc'] \n",
      "Chemin réel:\t ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-loc']\n",
      "0.0 % d'erreur\n",
      "\n",
      "\n",
      "La sentence : \n",
      "les prisonniers d' Al Qaïda de Guantanamo \n",
      "\n",
      "Chemin prédis:\t ['B-org', 'B-org', 'I-org', 'B-org', 'I-org', 'I-org', 'I-org'] \n",
      "Chemin réel:\t ['O', 'O', 'O', 'B-org', 'I-org', 'O', 'B-loc']\n",
      "71.43 % d'erreur\n",
      "\n",
      "\n",
      "La sentence : \n",
      "treize d' entre eux viennent d' être relâchés et ramenés en Afghanistan \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-332f48811dd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msent\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mviterbi\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"% d'erreur\\n\\n\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-7bf9daf6fb94>\u001b[0m in \u001b[0;36mviterbi\u001b[0;34m(unePhrase)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maViterbiProba\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maViterbiProba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mp_ath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "for sentence in devSplit:\n",
    "    print(\"La sentence : \")\n",
    "    sent = ''\n",
    "    for word in sentence :\n",
    "        sent += word[0] + \" \" \n",
    "    print(sent)\n",
    "    print( viterbi( sentence ), \"% d'erreur\\n\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
